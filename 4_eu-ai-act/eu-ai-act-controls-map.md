# EU AI Act Controls Map (High-Risk AI Systems)

> Translates EU AI Act obligations (Articles 9–15) into implementable controls for SMEs.
> Each row maps: obligation → control → evidence → owner → cadence → data governance dependency.

## How to use this controls map

1. Determine whether your AI system is classified as high-risk (see the high-risk requirements checklist).
2. For each applicable obligation below, implement the control described.
3. Produce the evidence listed and store it in the system's evidence pack.
4. Assign the owner within your organisation (adapt generic roles to your team structure).
5. Set the review cadence in your governance calendar and ensure reviews are completed on schedule.

This controls map covers provider and deployer obligations for high-risk AI systems under the EU AI Act. It does not cover general-purpose AI models (Article 53+) or prohibited practices (Article 5).

---

## Article 9 — Risk Management System

| Article | Obligation | Control | Evidence | Owner | Cadence | Data Governance Dependency |
|---|---|---|---|---|---|---|
| Art 9.1 | Establish a risk management system that operates continuously throughout the AI system's entire lifecycle | Define and document a risk management process covering identification, assessment, mitigation, monitoring, and review. Assign process ownership and integrate into the organisation's broader risk framework. | Risk management policy document specifying process steps, roles, escalation paths, and lifecycle integration points | AI Governance Lead | Annually (policy review) + at each lifecycle stage transition | None |
| Art 9.2(a), 9.2(b) | Identify and analyse known and reasonably foreseeable risks to health, safety, and fundamental rights — from both intended use and reasonably foreseeable misuse | Conduct a structured risk assessment for each AI use case at intake. Document identified risks with likelihood, severity, and affected population. Assess risks arising from intended use and from plausible misuse scenarios. | Completed risk assessment form with risk register entries. Each entry includes: risk description, likelihood rating, severity rating, affected rights/safety areas, and whether the risk arises from intended use or foreseeable misuse. | AI Governance Lead + System Owner | Per use case at intake. Reassess annually or when the system is materially changed. | Training data profiled (to identify data-related risks such as bias, gaps in representativeness, or poor quality inputs) |
| Art 9.2(c) | Evaluate risks that emerge after deployment based on post-market monitoring data | Establish a post-deployment risk review process. Analyse monitoring data, incident reports, user feedback, and performance drift to identify new or evolving risks not captured at intake. Update the risk register accordingly. | Updated risk register entries with post-deployment findings. Post-market monitoring review report documenting data sources analysed, new risks identified, and actions taken. | AI Governance Lead + Monitoring Lead | Quarterly post-deployment (minimum). Triggered additionally by incidents or significant performance changes. | Data quality measurement (monitoring data must be reliable). Data lineage documented (to trace issues back to root cause). |
| Art 9.2(d), 9.3 | Adopt appropriate risk management measures — prioritising elimination by design, then mitigation, then informing deployers of residual risk | For each identified risk, select and implement a risk management measure. Prioritise: (1) eliminate the risk through system redesign, (2) mitigate through technical or procedural controls, (3) accept the residual risk and document rationale. Test measures to confirm effectiveness. | Risk treatment plan documenting: each risk, the selected measure, rationale for the approach chosen (eliminate/mitigate/accept), and test results confirming effectiveness. Sign-off from risk owner. | AI Governance Lead + Technical Lead | Per use case (at risk assessment stage). Review when risk register is updated. | None |
| Art 9.4 | Test the AI system before placing on market or putting into service, using predefined metrics and probabilistic thresholds | Define testing criteria (accuracy, fairness, robustness metrics) with quantitative thresholds before testing begins. Execute tests against these criteria. Document pass/fail results. Do not deploy systems that fail defined thresholds without documented exception and additional mitigation. | Test plan with predefined metrics and thresholds. Test execution report with results. Pass/fail determination. Exception documentation if deployed despite a threshold breach (including additional mitigations applied). | Technical Lead + AI Governance Lead | Per use case before initial deployment. Repeat before any major system update or retraining. | Training data profiled (test validity depends on understanding data distribution). Data quality measurement (unreliable test data produces unreliable results). |
| Art 9.5 | Communicate residual risks to deployers with clear information on remaining limitations and risks | Produce a residual risk summary for each high-risk AI system. Include: risks that remain after mitigation, conditions under which residual risks may materialise, and recommended actions for the deployer. Deliver to deployer before or at deployment. | Residual risk communication document provided to deployer. Delivery confirmation (email, signed receipt, or equivalent). | AI Governance Lead | Per use case at deployment. Update when the risk register changes materially. | None |

---

## Article 10 — Data and Data Governance

| Article | Obligation | Control | Evidence | Owner | Cadence | Data Governance Dependency |
|---|---|---|---|---|---|---|
| Art 10.2(a) | Training, validation, and testing data must be subject to appropriate data governance and management practices — including design choices for data collection | Document design choices for data collection: what data is collected, from which sources, why those sources were chosen, and what alternatives were considered. Record sampling strategy and any exclusion criteria. | Data collection design document specifying sources, rationale, sampling approach, and exclusion criteria | Data Owner + Technical Lead | Per use case at data collection stage. Update when data sources change. | Data catalogued (all data sources registered and discoverable). Data lineage documented (provenance of each dataset recorded). |
| Art 10.2(b), 10.2(c) | Data preparation and processing must be documented, including annotation, labelling, cleaning, and enrichment | Define and document all data preparation steps: cleaning rules, labelling methodology (including annotator guidelines and qualifications), enrichment processes, and transformation logic. Make the pipeline reproducible. | Data preparation log documenting each processing step, tools used, transformation rules applied, and labelling methodology. Annotator guidelines if human labelling is involved. | Data Owner + Technical Lead | Per use case at data preparation stage. Update when pipeline changes. | Data quality measurement (quality checks at each preparation step). Data lineage documented (full transformation chain from raw to processed). |
| Art 10.2(f) | Examine training data for possible biases that may affect health, safety, or lead to discrimination | Conduct a bias assessment on training data before model training. Assess representation across protected characteristics (age, gender, ethnicity, disability) relevant to the system's domain. Document findings and any corrective actions. | Bias assessment report documenting: characteristics assessed, methodology used, findings (including under/over-representation), and corrective actions taken (resampling, augmentation, exclusion). | Data Owner + AI Governance Lead | Per use case before initial training. Repeat before any retraining with new data. | Training data profiled (demographic and characteristic distributions analysed). Data quality measurement (completeness and accuracy of demographic attributes). |
| Art 10.2(g) | Identify relevant data gaps or shortcomings and how to address them | Assess whether training data adequately covers the intended operating conditions and population. Identify gaps in coverage, edge cases, or underrepresented scenarios. Document how gaps are addressed (additional data collection, synthetic augmentation, or acknowledged as a limitation). | Data gap analysis report documenting: coverage assessment against intended use, identified gaps, actions taken to address gaps, and residual gaps communicated as system limitations. | Data Owner + Technical Lead | Per use case before initial training. Reassess when intended use scope changes. | Data catalogued (to identify what data is available vs. what is needed). Training data profiled (to identify distributional gaps). |
| Art 10.3 | Training, validation, and testing datasets must be relevant, sufficiently representative, and to the best extent possible free of errors and complete | Define data quality criteria for each dataset: relevance to intended purpose, representativeness of the target population, error rate thresholds, and completeness targets. Measure datasets against these criteria before use. | Data quality assessment report documenting: defined quality criteria, measurement results for each criterion, pass/fail determination, and remediation actions for any criterion that failed. | Data Owner | Per use case before training. Repeat before retraining. Continuous monitoring of live data quality post-deployment. | Data quality measurement (defined metrics for accuracy, completeness, consistency, timeliness, validity, uniqueness). Data catalogued (datasets registered with quality metadata). |
| Art 10.5 | Where strictly necessary for bias monitoring, detection, and correction, providers may process special categories of personal data subject to safeguards | If bias detection requires processing of special category data (ethnicity, health, religion, etc.), document the legal basis, necessity assessment, and safeguards applied. Ensure GDPR Article 9 conditions are met. Limit processing to what is strictly necessary. | Necessity assessment for special category data processing. GDPR Article 9 legal basis documentation. Safeguards register (pseudonymisation, access controls, retention limits, purpose limitation). | Data Owner + DPO/Legal | Per use case where special category data is processed. Review annually. | Data classification (special category data identified and tagged). Data catalogued (processing purposes and legal basis recorded per dataset). |

---

## Article 11 — Technical Documentation

| Article | Obligation | Control | Evidence | Owner | Cadence | Data Governance Dependency |
|---|---|---|---|---|---|---|
| Art 11.1 | Technical documentation must be drawn up before the system is placed on the market or put into service, and kept up to date | Produce technical documentation covering all elements specified in Annex IV: system description, intended purpose, design specifications, development methodology, data requirements, performance metrics, risk management measures, and post-market monitoring plan. Complete documentation before deployment. | Technical documentation package aligned to Annex IV. Document version control log showing it was created before deployment and updated after material changes. | Technical Lead + AI Governance Lead | Created before initial deployment. Updated within 30 days of any material change to the system. | Data lineage documented (required for Annex IV data description sections). Training data profiled (required for Annex IV dataset description). |
| Art 11.2 | Where a high-risk AI system is related to a product covered by other Union harmonisation legislation, a single set of technical documentation may be drawn up | Assess whether the AI system is embedded in a product subject to other EU legislation (e.g., medical devices, machinery, toys). If so, integrate AI Act technical documentation requirements into the existing product documentation rather than duplicating. | Regulatory applicability assessment identifying all applicable EU legislation. Integrated technical documentation or cross-reference matrix showing where AI Act requirements are addressed within product documentation. | AI Governance Lead + Regulatory Lead | At system design stage. Review when regulatory landscape changes. | None |

---

## Article 12 — Record-Keeping

| Article | Obligation | Control | Evidence | Owner | Cadence | Data Governance Dependency |
|---|---|---|---|---|---|---|
| Art 12.1 | High-risk AI systems must allow for automatic recording of events (logs) throughout the system's lifetime | Design the system with automatic logging enabled for key events: inputs received, outputs generated, decisions made, confidence scores, errors, anomalies, and operator interactions. Ensure logs are generated without manual intervention. | System design specification confirming logging capability. Sample log export demonstrating events are captured automatically. Log schema documentation. | Technical Lead | At system design stage. Verify logging is active at each deployment. | None (this is a system design requirement) |
| Art 12.2, 12.3 | Logging must enable traceability and monitoring of system functioning, must be retained for an appropriate period | Define log retention policy specifying: what is logged, retention period (minimum duration appropriate to intended purpose and applicable law), storage location, access controls, and deletion schedule. Ensure logs are sufficient to reconstruct the system's decision process for any given output. | Log retention policy document. Evidence of log storage with access controls. Demonstration that logs can reconstruct a specific decision (sample trace from input to output). | Technical Lead + AI Governance Lead | Annually (policy review). Continuous (logging operation). | Data quality measurement (log data must be complete and uncorrupted). Data lineage documented (ability to trace decisions requires linked data chain). 

---

## Article 13 — Transparency and Provision of Information to Deployers

| Article | Obligation | Control | Evidence | Owner | Cadence | Data Governance Dependency |
|---|---|---|---|---|---|---|
| Art 13.1 | High-risk AI systems must be designed and developed to ensure their operation is sufficiently transparent for deployers to interpret output and use it appropriately | Produce deployer-facing documentation covering: system capabilities and limitations, intended purpose, input data requirements, output interpretation guidance, known failure modes, and conditions under which performance degrades. Use clear, non-technical language where the deployer is a non-specialist. | Deployer documentation or system card containing: intended purpose, capabilities, known limitations, performance benchmarks, input requirements, output interpretation guide, and failure mode descriptions. | AI Governance Lead + Technical Lead | Created before deployment. Updated when system behaviour changes materially. | None |
| Art 13.3(b)(ii) | Instructions for use must include the level of accuracy, robustness, and cybersecurity, and any known or foreseeable circumstances that may impact performance | Include in deployer documentation: declared accuracy metrics with test conditions, robustness testing results, cybersecurity assessment summary, and specific circumstances known to affect performance (e.g., data drift, adversarial inputs, edge case populations). | Accuracy and performance declaration within deployer documentation. Robustness test summary. Cybersecurity assessment summary. List of known performance-affecting circumstances with mitigation guidance. | Technical Lead | Created before deployment. Updated when performance metrics change after retraining or system update. | Training data profiled (performance metrics depend on understanding data characteristics). Data quality measurement (declared accuracy is only meaningful if test data quality is known). |

---

## Article 14 — Human Oversight

| Article | Obligation | Control | Evidence | Owner | Cadence | Data Governance Dependency |
|---|---|---|---|---|---|---|
| Art 14.1, 14.2 | High-risk AI systems must be designed to allow effective human oversight during use, including the ability to fully understand capabilities and limitations, monitor operation, and interpret outputs | Design the system with a human oversight interface that enables an operator to: view system outputs and confidence levels, understand how the system reached a decision, monitor performance in real time, and access all relevant contextual information needed to evaluate output. | Human oversight design specification. Screenshots or description of oversight interface. Operator user guide documenting how to interpret outputs and monitor performance. | Technical Lead + AI Governance Lead | At system design stage. Review at each major system update. | None |
| Art 14.3(d) | Humans must be able to decide not to use the system, override or reverse its output, or intervene in or stop the system's operation | Implement technical controls enabling an authorised operator to: override individual system outputs, intervene in the system's operation, and stop the system entirely. Document the override/stop procedure and ensure it is accessible without specialist technical knowledge. | Override and stop procedure document. Technical specification confirming override/stop controls exist. Test evidence demonstrating override and stop functions work as designed (test log with date). | Technical Lead | At system design stage. Test override/stop controls before each deployment. Retest after major updates. | None |
| Art 14.4 | Persons assigned to human oversight must have the competence, training, and authority necessary to fulfil that role | Identify personnel responsible for human oversight for each high-risk AI system. Provide training covering: system capabilities and limitations, how to interpret outputs, when and how to intervene, escalation procedures, and regulatory context. Document completion and refresh training periodically. | Training programme specification. Training completion records per individual. Competence assessment results. Role assignment document confirming oversight authority. | AI Governance Lead + HR/People Lead | Training before deployment. Refresher annually. Update training when system changes materially. | None |

---

## Article 15 — Accuracy, Robustness and Cybersecurity

| Article | Obligation | Control | Evidence | Owner | Cadence | Data Governance Dependency |
|---|---|---|---|---|---|---|
| Art 15.1 | High-risk AI systems must achieve an appropriate level of accuracy, robustness, and cybersecurity throughout their lifecycle | Define minimum acceptable levels for accuracy, robustness, and cybersecurity at system design stage. Document these as measurable thresholds. Establish ongoing measurement and monitoring to ensure levels are maintained throughout the system's lifecycle, not just at deployment. | Accuracy, robustness, and cybersecurity threshold specification. Baseline measurement results at deployment. Ongoing measurement results from post-deployment monitoring. Remediation records when thresholds are breached. | Technical Lead + AI Governance Lead | Thresholds set at design stage. Baseline measured before deployment. Ongoing monitoring continuous or quarterly minimum. | Data quality measurement (accuracy metrics are only valid if measured against quality-assured test data). |
| Art 15.2 | Accuracy levels and relevant accuracy metrics must be declared in accompanying instructions for use | Include declared accuracy metrics in deployer documentation (see Art 13.3). Specify: metric name (e.g., F1 score, AUC, precision, recall), value achieved, test conditions, and test dataset description. Clearly state what the metrics mean in practical terms for the deployer. | Accuracy declaration within deployer documentation specifying: metrics used, values achieved, test conditions, test dataset description, and plain-language interpretation for non-technical deployers. | Technical Lead | Before deployment. Update after retraining or when accuracy materially changes. | Training data profiled (deployer needs to understand what data the accuracy was measured against). |
| Art 15.3 | High-risk AI systems must be resilient to errors, faults, or inconsistencies in input data and to attempts by unauthorised third parties to manipulate the system | Conduct robustness testing covering: noisy or incomplete input data, adversarial inputs, edge cases, and unexpected data formats. Conduct security testing covering: adversarial attacks, data poisoning, model extraction, and prompt injection (where applicable). Document results and implement mitigations for identified vulnerabilities. | Robustness test report documenting: test scenarios, results, and mitigations applied. Security assessment report documenting: threat model, vulnerabilities identified, penetration test results (if applicable), and mitigations applied. | Technical Lead + Security Lead | Before deployment. Repeat annually. Repeat after material system changes. Triggered by security incidents. | Data quality measurement (robustness testing requires controlled data inputs). |
| Art 15.5 | High-risk AI systems must be resilient against attempts to alter their use, outputs, or performance by exploiting system vulnerabilities | Implement cybersecurity controls appropriate to the system's risk level. As a minimum: access controls for model and training data, integrity monitoring for model weights, input validation, audit logging of access and changes, and incident response procedure for security breaches. | Cybersecurity controls register documenting: controls implemented, coverage assessment, and residual risks. Access control configuration evidence. Integrity monitoring setup evidence. Incident response procedure for AI-specific security events. | Security Lead + Technical Lead | Before deployment. Review annually. Triggered by security incidents or vulnerability disclosures. | Data lineage documented (to detect unauthorised changes to training data). Data quality measurement (to detect data poisoning via quality degradation). |

---

## Limitations

This controls map is designed for SMEs deploying or procuring high-risk AI systems. It does not cover:

- Provider-side conformity assessment procedures (Annex VI / VII)
- General-purpose AI model obligations (Article 53+)
- Prohibited AI practices (Article 5)
- Sector-specific obligations (e.g. medical device regulation, financial services prudential requirements)

Controls should be adapted to the organisation's size, risk appetite, and regulatory context. This map provides a starting structure, not a definitive compliance programme.
